{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "speaker-verification-using-siamese-network.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "yslH5PtB5YLH",
        "colab_type": "code",
        "outputId": "9168633e-1c63-4f0d-c2ff-e7d4ed5b4abd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4BLdItx66Rlv",
        "colab_type": "code",
        "outputId": "6de6c39a-6000-46e4-d427-d4c18524e0c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install librosa"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (0.6.3)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (2.1.6)\n",
            "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.16.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.2.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.20.3)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.12.5)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.4.0)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.11.0)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.2.1)\n",
            "Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.40.1)\n",
            "Requirement already satisfied: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (0.28.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dY8NyBq06SFZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import pickle\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kqoi5cc16T_P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pr3Fh6Z96wVk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Read and transform data\n",
        "\n",
        "Convert raw byte stream into useful feature space. STFT on the signals serves the initial feature extraction process"
      ]
    },
    {
      "metadata": {
        "id": "IvDw-ijN5v0G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "base_path = './drive/My Drive/Colab Notebooks/hw4/speaker-verification/'\n",
        "train_path = base_path + 'hw4_trs.pkl'\n",
        "test_path = base_path + 'hw4_tes.pkl'\n",
        "\n",
        "base_path_model = base_path + 'model/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PpNd1Zi56bi5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(train_path, 'rb') as f:\n",
        "  train_raw = pickle.load(f)\n",
        "\n",
        "with open(test_path, 'rb') as f:\n",
        "  test_raw = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R2hLaf5X65hB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "trs.pkl contains an 500×16,180 matrix, whose row is a speech signal with 16,180 samples. They are\n",
        "the returned vectors from the librosa.load function. Similarly, tes.pkl holds a 200×22,631 matrix"
      ]
    },
    {
      "metadata": {
        "id": "ttwIhH6i66EI",
        "colab_type": "code",
        "outputId": "43bb9bab-5454-4924-94fb-c9d95d5243f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_raw.shape, test_raw.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((500, 16180), (200, 22631))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "R2BM7QsV7WYD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Convert into audio (.wav file) for checking manually"
      ]
    },
    {
      "metadata": {
        "id": "9H81tr6D7b6k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# for i in range(train_raw.shape[0]):\n",
        "#   librosa.output.write_wav(base_path + 'trs/trs' + ('0000' + str(i))[-4:] + '.wav', train_raw[i], 16000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ps_uKZeM8NhN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# for i in range(test_raw.shape[0]):\n",
        "#   librosa.output.write_wav(base_path + 'tes/tes' + ('0000' + str(i))[-4:] + '.wav', test_raw[i], 16000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-hm8h0p86mF4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Extract features using STFT"
      ]
    },
    {
      "metadata": {
        "id": "wsiuXvox6meZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_complex = np.array([librosa.stft(x, n_fft=1024, hop_length=512).T for x in train_raw])\n",
        "test_complex = np.array([librosa.stft(x, n_fft=1024, hop_length=512).T for x in test_raw])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s65BSNaT4XTZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = np.abs(train_complex)\n",
        "test = np.abs(test_complex)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uRw8RQBm9xZ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5c82360c-54f3-4dce-d030-5379b5c8e87d"
      },
      "cell_type": "code",
      "source": [
        "a = [[1, 2], [3, 4]]\n",
        "np.pad()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2], [3, 4]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "WrdcMtTd57Mp",
        "colab_type": "code",
        "outputId": "f9712454-5086-4c0f-8683-5a9a8003c062",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train.shape, test.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((500, 32, 513), (200, 45, 513))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "yGoPCmQw88_Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Generate mini-batches"
      ]
    },
    {
      "metadata": {
        "id": "EcDhqpcu8chH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Structure of training data\n",
        "\n",
        "The training matrix is ordered by speakers. Each speaker has 10 utterances, and there are 50 such\n",
        "speakers (that’s why there are 500 rows). Similarly, the test set has 20 speakers, each of which is with\n",
        "10 utterances."
      ]
    },
    {
      "metadata": {
        "id": "xoPLr-ACE13o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "samples_per_class = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-xqSBYCI7LCq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Procedure to generate mini-batches\n",
        "\n",
        "1. Randomly sample L pairs of utterances from the ten utterance of the first speaker. In theory, there are $10 \\choose 2$= 45 pairs you can sample from. You can use all 45 of them if you want. These are the positive examples in your first minibatch\n",
        "\n",
        "2. Randomly sample L utterances from the 49 training speakers. Using them and the ten utterances of\n",
        "the first speaker, form another set of L pairs. If L > 10, you’ll need to repeatedly use the first speaker’s\n",
        "utterance (i.e. sampling with replacement). This set is your negative examples, each of whose pair\n",
        "contains an utterance from the first speaker and a random utterance spoken by a different speaker.\n",
        "\n",
        "3. In this first minibatch, you have 2L pairs of utterances.\n",
        "\n",
        "4. Repeat this process for the other training speakers, so that each speaker is represented by L positive\n",
        "pairs and L negative pairs. By doing so, you can form 50 minibatches with a balanced number of\n",
        "positive and negative pair"
      ]
    },
    {
      "metadata": {
        "id": "JwBYGb-i5iQ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_length = 50\n",
        "num_features = 513"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cFDD7y3b5DXT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pad_zeros(stft):\n",
        "  stft_val = np.zeros((max_length, num_features))\n",
        "  stft_val[:stft.shape[0], :stft.shape[1]] = stft\n",
        "  return stft_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fNIWxX-OAOAC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "data: train/test\n",
        "batch_size: return batch_size number of positive and another batch_size number of negative pairs\n",
        "stick: if true, one training class will be common across positive and negative pairs\n",
        "\n",
        "returns:\n",
        "x: list of pairs\n",
        "y: 1 for positive pairs, 0 for negative\n",
        "'''\n",
        "def next_batch(data, batch_size, stick=False):\n",
        "  x1, x2, l1, l2, y = [], [], [], [], []\n",
        "  \n",
        "  num_classes = len(data) // samples_per_class\n",
        "  base = np.random.randint(num_classes)\n",
        "  \n",
        "  # generate positive pairs\n",
        "  for _ in range(batch_size):\n",
        "    # randomly select idx_0 only if stick is false\n",
        "    if not stick:\n",
        "      base = np.random.randint(num_classes)\n",
        "      \n",
        "    idx_0, idx_1 = base * samples_per_class + np.random.choice(np.arange(samples_per_class), size=2, replace=False)\n",
        "    x1.append(pad_zeros(data[idx_0]))\n",
        "    x2.append(pad_zeros(data[idx_1]))\n",
        "    l1.append(data[idx_0].shape[0])\n",
        "    l2.append(data[idx_1].shape[0])\n",
        "    y.append([1])\n",
        "#     print(idx_0, idx_1, 1)\n",
        "    \n",
        "  # generate negative pairs\n",
        "  for _ in range(batch_size):\n",
        "    # randomly select idx_0 only if stick is false\n",
        "    if not stick:\n",
        "      base = np.random.randint(num_classes)\n",
        "    \n",
        "    # make sure neg_base is not same as base\n",
        "    while True:\n",
        "      neg_base = np.random.randint(num_classes)\n",
        "      if neg_base != base:\n",
        "        break\n",
        "      \n",
        "    idx_0 = base * samples_per_class + np.random.randint(samples_per_class)\n",
        "    idx_1 = neg_base * samples_per_class + np.random.randint(samples_per_class)\n",
        "    \n",
        "    x1.append(pad_zeros(train[idx_0]))\n",
        "    x2.append(pad_zeros(train[idx_1]))\n",
        "    l1.append(train[idx_0].shape[0])\n",
        "    l2.append(train[idx_1].shape[0])\n",
        "    y.append([0])\n",
        "#     print(idx_0, idx_1, 0)\n",
        "    \n",
        "  return np.array(x1), np.array(x2), np.array(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rVVbymY8cuAx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Create model"
      ]
    },
    {
      "metadata": {
        "id": "K2E9VhP7uqxf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_hidden = 256\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yDRUnq3hqgkN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Using this implementation as reference https://github.com/ardiya/siamesenetwork-tensorflow/blob/master/train.py\n",
        "\n",
        "def model(inp, reuse):\n",
        "  with tf.name_scope('model'):\n",
        "    \n",
        "    with tf.variable_scope('lstm_1') as scope:\n",
        "      cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_hidden, reuse=reuse))\n",
        "      output, state = tf.nn.dynamic_rnn(cell, inp, dtype=tf.float32, scope=scope)\n",
        "      \n",
        "#     with tf.variable_scope('conv_1') as scope:\n",
        "#       conv_1 = tf.contrib.layers.conv2d(inp, 16, [1, 3], activation_fn=tf.nn.relu, padding='same', \\\n",
        "# \t\t        weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(), scope=scope, reuse=reuse)\n",
        "#       pool_1 = tf.contrib.layers.max_pool2d(conv_1, [1, 2], stride=[1, 2])\n",
        "#       flattened_1 = tf.contrib.layers.flatten(pool_1)\n",
        "#       dropout_1 = tf.contrib.layers.dropout(flattened_1, keep_prob=0.9)\n",
        "      \n",
        "    with tf.variable_scope('dense_1') as scope:\n",
        "      flattened_1 = tf.contrib.layers.flatten(output)\n",
        "      dropout_1 = tf.contrib.layers.dropout(flattened_1, keep_prob=0.9)\n",
        "      dense_1 = tf.contrib.layers.fully_connected(dropout_1, 200, activation_fn=tf.nn.sigmoid, \\\n",
        "            weights_initializer=tf.contrib.layers.xavier_initializer(), scope=scope, reuse=reuse)\n",
        "     \n",
        "      connected = tf.layers.Dense(256)(dense_1)\n",
        "      notConnected = tf.layers.Dense(256)\n",
        "      \n",
        "      print(connected)\n",
        "      print(notConnected)\n",
        "  return dense_1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xCp67YQZDMnw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loss function"
      ]
    },
    {
      "metadata": {
        "id": "RrBRDdVunc4u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def contrastive_loss(model1, model2, y, margin):\n",
        "  # y can take values in {0, 1}\n",
        "  \n",
        "  with tf.name_scope('contrastive-loss'):\n",
        "#     distance = tf.sqrt(tf.reduce_sum(tf.pow(model1 - model2, 2), 1, keepdims=True))\n",
        "#     similarity = y * tf.square(distance)\n",
        "    \n",
        "#     # distance between different object should be at least 'margin'\n",
        "#     dissimilarity = (1 - y) * tf.square(tf.maximum((margin - distance), 0))\n",
        "    \n",
        "#     return tf.reduce_mean(dissimilarity + similarity) / 2\n",
        "    print(model1)\n",
        "    print(model2)\n",
        "    print(y)\n",
        "\n",
        "    dotProduct = tf.reduce_sum(tf.multiply(model1, model2), axis = 1, keepdims=True)\n",
        "    sigmDotProduct = tf.nn.sigmoid(dotProduct)\n",
        "    notConnected = tf.contrib.layers.fully_connected(dropout_1, 200, activation_fn=tf.nn.sigmoid, \\\n",
        "            weights_initializer=tf.contrib.layers.xavier_initializer(), scope=scope, reuse=reuse)\n",
        "    lossCalcu  = tf.reduce_sum(-y*tf.log(10e-8 + sigmDotProduct) - (1 - y) * tf.log(10e-8 + 1 - sigmDotProduct))\n",
        "    print(dotProduct)\n",
        "    print(sigmDotProduct)\n",
        "    print(lossCalcu)\n",
        "    return lossCalcu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hFmScMV40XHB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def similar(model1, model2, y):\n",
        "  distance = tf.sqrt(tf.reduce_sum(tf.pow(model1 - model2, 2), 1, keepdims=True))\n",
        "  prob = 1 - tf.nn.sigmoid(distance)\n",
        "  prediction = tf.cast(tf.greater_equal(prob, 0.5), tf.float32)\n",
        "  accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, y), tf.float32))\n",
        "  return prob, prediction, accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1ewIvmzHF2UA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Inputs to the model"
      ]
    },
    {
      "metadata": {
        "id": "cWtiQaylvL-1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X1 = tf.placeholder(dtype='float', shape=[None, max_length, num_features])\n",
        "X2 = tf.placeholder(dtype='float', shape=[None, max_length, num_features])\n",
        "y = tf.placeholder(dtype='float', shape=[None, 1])\n",
        "margin = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3kljT8GaF9S2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Plug everything together"
      ]
    },
    {
      "metadata": {
        "id": "qcAm-hvPwqre",
        "colab_type": "code",
        "outputId": "923d5b4a-052f-472a-eabd-e90300f429b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "cell_type": "code",
      "source": [
        "model1 = model(X1, None)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-18-557aeae97701>:6: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-18-557aeae97701>:7: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SGXsVU373-W3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5cf1455a-758f-4ded-f1f4-a3f55fbc1945"
      },
      "cell_type": "code",
      "source": [
        "model2 = model(X2, True)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"model_2/dense_1/dense/BiasAdd:0\", shape=(?, 256), dtype=float32)\n",
            "<tensorflow.python.layers.core.Dense object at 0x7f6fa15ef7f0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fnYFf5v82zMZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "7a5f6d4e-c90a-47d4-a63d-7b432bee929d"
      },
      "cell_type": "code",
      "source": [
        "loss = contrastive_loss(model1, model2, y, margin)\n",
        "train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss=loss)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"model/dense_1/dense_1/Sigmoid:0\", shape=(?, 200), dtype=float32)\n",
            "Tensor(\"model_1/dense_1/dense_1/Sigmoid:0\", shape=(?, 200), dtype=float32)\n",
            "Tensor(\"Placeholder_2:0\", shape=(?, 1), dtype=float32)\n",
            "Tensor(\"contrastive-loss/Sum:0\", shape=(?, 1), dtype=float32)\n",
            "Tensor(\"contrastive-loss/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
            "Tensor(\"contrastive-loss/Sum_1:0\", shape=(), dtype=float32)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XiBMTxV_IH9v",
        "colab_type": "code",
        "outputId": "820e9b58-ff8d-4d3d-815f-6c658e7fa3b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(model1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"model/dense_1/dense_1/Sigmoid:0\", shape=(?, 200), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DX6_tAMqIfCG",
        "colab_type": "code",
        "outputId": "9006b123-1ef1-49ea-8a6e-ee1b6d7e7ef5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"contrastive-loss/Sum_1:0\", shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sUYM0mghGEhb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ]
    },
    {
      "metadata": {
        "id": "keHumcYzrxBS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "num_epochs = 50\n",
        "display_step = 10\n",
        "save_step = 10\n",
        "\n",
        "num_samples_tr = train.shape[0]\n",
        "num_samples_te = test.shape[0]\n",
        "num_batches_tr = int(math.ceil(num_samples_tr/batch_size))\n",
        "num_batches_te = int(math.ceil(num_samples_te/batch_size))\n",
        "\n",
        "train_loss = []\n",
        "test_loss = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C1ViimzPsGvJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AF8-0ioesIc3",
        "colab_type": "code",
        "outputId": "6431320d-ea71-4d1d-947c-5aef0bdfbe2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 999
        }
      },
      "cell_type": "code",
      "source": [
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "num_batches_tr = num_samples_tr // batch_size\n",
        "for epoch in range(num_epochs):\n",
        "  loss_val = 0\n",
        "  for i in range(num_batches_tr):\n",
        "    batch_x1, batch_x2, batch_y = next_batch(train, batch_size)\n",
        "    \n",
        "    _, lv = sess.run([train_op, loss], feed_dict={X1: batch_x1, X2: batch_x2, y: batch_y})\n",
        "    loss_val += lv\n",
        "    \n",
        "  loss_val = loss_val / (num_batches_tr * batch_size)\n",
        "    \n",
        "  if epoch % display_step == 0:\n",
        "    print(epoch, loss_val)\n",
        "  if epoch % save_step == 0:\n",
        "    saver.save(sess, base_path_model + 'model_' + str(epoch) + '.ckpt')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 15.942385864257812\n",
            "10 15.942385864257812\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-63e696803486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbatch_x1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_x2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mloss_val\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "7t5VQBfH1D3O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test model"
      ]
    },
    {
      "metadata": {
        "id": "NRB122xNGPku",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_x1, batch_x2, batch_y = next_batch(test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wo9yVwpa0Lg2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prob, prediction, accuracy = similar(model1, model2, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3w_LpP3n04cw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pb, pd, ac = sess.run([prob, prediction, accuracy], feed_dict={X1: batch_x1, X2: batch_x2, y: batch_y})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NfIMjC0u1x2T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R2r04y1q2Bmf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pd, batch_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2IEXudrc4V1P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ac"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eqPzzuVF4WEo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}