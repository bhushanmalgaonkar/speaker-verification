{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "speaker-verification-using-siamese-network - controstive loss.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "yslH5PtB5YLH",
        "colab_type": "code",
        "outputId": "a426fd2e-da35-4c40-8dc1-10b06898e641",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4BLdItx66Rlv",
        "colab_type": "code",
        "outputId": "94d7a56b-568c-43f3-fcb8-ed5c6efc2cb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install librosa"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (0.6.3)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (2.1.6)\n",
            "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.16.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.2.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.20.3)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.12.5)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.4.0)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.11.0)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.2.1)\n",
            "Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.40.1)\n",
            "Requirement already satisfied: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (0.28.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hBaWBlEJsBc6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "22e23e18-836f-4840-a424-45d6e41aad0f"
      },
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-19 02:18:19--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.173.32.212, 35.173.6.94, 52.203.66.95, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.173.32.212|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14977695 (14M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.6’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  14.28M  18.1MB/s    in 0.8s    \n",
            "\n",
            "2019-04-19 02:18:20 (18.1 MB/s) - ‘ngrok-stable-linux-amd64.zip.6’ saved [14977695/14977695]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fvLYML8GsFu8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LOG_DIR = './log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y6ZMO77vsK2N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NbV0qsZCsOFr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f817f165-d951-4739-97db-191981c408e9"
      },
      "cell_type": "code",
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://715d4a2f.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dY8NyBq06SFZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import pickle\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kqoi5cc16T_P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pr3Fh6Z96wVk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Read and transform data\n",
        "\n",
        "Convert raw byte stream into useful feature space. STFT on the signals serves the initial feature extraction process"
      ]
    },
    {
      "metadata": {
        "id": "IvDw-ijN5v0G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "base_path = './drive/My Drive/Colab Notebooks/hw4/speaker-verification/'\n",
        "train_path = base_path + 'hw4_trs.pkl'\n",
        "test_path = base_path + 'hw4_tes.pkl'\n",
        "\n",
        "base_path_model = base_path + 'model/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PpNd1Zi56bi5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(train_path, 'rb') as f:\n",
        "  train_raw = pickle.load(f)\n",
        "\n",
        "with open(test_path, 'rb') as f:\n",
        "  test_raw = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R2hLaf5X65hB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "trs.pkl contains an 500×16,180 matrix, whose row is a speech signal with 16,180 samples. They are\n",
        "the returned vectors from the librosa.load function. Similarly, tes.pkl holds a 200×22,631 matrix"
      ]
    },
    {
      "metadata": {
        "id": "ttwIhH6i66EI",
        "colab_type": "code",
        "outputId": "fc466748-9e9f-4afa-bd3f-102f71ff5e16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_raw.shape, test_raw.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((500, 16180), (200, 22631))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "R2BM7QsV7WYD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Convert into audio (.wav file) for checking manually"
      ]
    },
    {
      "metadata": {
        "id": "9H81tr6D7b6k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# for i in range(train_raw.shape[0]):\n",
        "#   librosa.output.write_wav(base_path + 'trs/trs' + ('0000' + str(i))[-4:] + '.wav', train_raw[i], 16000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ps_uKZeM8NhN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# for i in range(test_raw.shape[0]):\n",
        "#   librosa.output.write_wav(base_path + 'tes/tes' + ('0000' + str(i))[-4:] + '.wav', test_raw[i], 16000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-hm8h0p86mF4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Extract features using STFT"
      ]
    },
    {
      "metadata": {
        "id": "wsiuXvox6meZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_complex = np.array([librosa.stft(x, n_fft=1024, hop_length=512).T for x in train_raw])\n",
        "test_complex = np.array([librosa.stft(x, n_fft=1024, hop_length=512).T for x in test_raw])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s65BSNaT4XTZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = np.abs(train_complex)\n",
        "test = np.abs(test_complex)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uRw8RQBm9xZ4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# a = [[1, 2], [3, 4]]\n",
        "# np.pad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WrdcMtTd57Mp",
        "colab_type": "code",
        "outputId": "9dd34150-b613-46d9-9670-6e76078a602c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train.shape, test.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((500, 32, 513), (200, 45, 513))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "yGoPCmQw88_Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Generate mini-batches"
      ]
    },
    {
      "metadata": {
        "id": "EcDhqpcu8chH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Structure of training data\n",
        "\n",
        "The training matrix is ordered by speakers. Each speaker has 10 utterances, and there are 50 such\n",
        "speakers (that’s why there are 500 rows). Similarly, the test set has 20 speakers, each of which is with\n",
        "10 utterances."
      ]
    },
    {
      "metadata": {
        "id": "xoPLr-ACE13o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "samples_per_class = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-xqSBYCI7LCq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Procedure to generate mini-batches\n",
        "\n",
        "1. Randomly sample L pairs of utterances from the ten utterance of the first speaker. In theory, there are $10 \\choose 2$= 45 pairs you can sample from. You can use all 45 of them if you want. These are the positive examples in your first minibatch\n",
        "\n",
        "2. Randomly sample L utterances from the 49 training speakers. Using them and the ten utterances of\n",
        "the first speaker, form another set of L pairs. If L > 10, you’ll need to repeatedly use the first speaker’s\n",
        "utterance (i.e. sampling with replacement). This set is your negative examples, each of whose pair\n",
        "contains an utterance from the first speaker and a random utterance spoken by a different speaker.\n",
        "\n",
        "3. In this first minibatch, you have 2L pairs of utterances.\n",
        "\n",
        "4. Repeat this process for the other training speakers, so that each speaker is represented by L positive\n",
        "pairs and L negative pairs. By doing so, you can form 50 minibatches with a balanced number of\n",
        "positive and negative pair"
      ]
    },
    {
      "metadata": {
        "id": "JwBYGb-i5iQ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_length = 50\n",
        "num_features = 513"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cFDD7y3b5DXT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pad_zeros(stft):\n",
        "  stft_val = np.zeros((max_length, num_features))\n",
        "  stft_val[:stft.shape[0], :stft.shape[1]] = stft\n",
        "  return stft_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AyAXVOftV7UA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "data: train/test\n",
        "batch_size: return batch_size number of positive and another batch_size number of negative pairs\n",
        "stick: if true, one training class will be common across positive and negative pairs\n",
        "\n",
        "returns:\n",
        "x: list of pairs\n",
        "y: 1 for positive pairs, 0 for negative\n",
        "'''\n",
        "def next_batch_triplet(data, batch_size, stick=False):\n",
        "  xa, xp, xn, l = [], [], [], []\n",
        "  \n",
        "  num_classes = len(data) // samples_per_class\n",
        "  base = np.random.randint(num_classes)\n",
        "  \n",
        "  # generate positive pairs\n",
        "  for _ in range(batch_size):\n",
        "    # randomly select idx_0 only if stick is false\n",
        "    if not stick:\n",
        "      base = np.random.randint(num_classes)\n",
        "      \n",
        "    # make sure neg_base is not same as base\n",
        "    while True:\n",
        "      neg_base = np.random.randint(num_classes)\n",
        "      if neg_base != base:\n",
        "        break\n",
        "      \n",
        "    idx_a, idx_p = base * samples_per_class + np.random.choice(np.arange(samples_per_class), size=2, replace=False)\n",
        "    idx_n = neg_base * samples_per_class + np.random.randint(samples_per_class)\n",
        "    \n",
        "    xa.append(pad_zeros(data[idx_a]))\n",
        "    xp.append(pad_zeros(data[idx_p]))\n",
        "    xn.append(pad_zeros(data[idx_n]))\n",
        "    \n",
        "#     xa.append(data[idx_a])\n",
        "#     xp.append(data[idx_p])\n",
        "#     xn.append(data[idx_n])\n",
        "    \n",
        "    l.append(data[idx_a].shape[0])\n",
        "    \n",
        "  return np.array(xa), np.array(xp), np.array(xn), np.array(l)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fNIWxX-OAOAC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "data: train/test\n",
        "batch_size: return batch_size number of positive and another batch_size number of negative pairs\n",
        "stick: if true, one training class will be common across positive and negative pairs\n",
        "\n",
        "returns:\n",
        "x: list of pairs\n",
        "y: 1 for positive pairs, 0 for negative\n",
        "'''\n",
        "def next_batch(data, batch_size, stick=False):\n",
        "  x1, x2, y, l1, l2 = [], [], [], [], []\n",
        "  \n",
        "  num_classes = len(data) // samples_per_class\n",
        "  base = np.random.randint(num_classes)\n",
        "  \n",
        "  # generate positive pairs\n",
        "  for _ in range(batch_size):\n",
        "    # randomly select idx_0 only if stick is false\n",
        "    if not stick:\n",
        "      base = np.random.randint(num_classes)\n",
        "      \n",
        "    idx_0, idx_1 = base * samples_per_class + np.random.choice(np.arange(samples_per_class), size=2, replace=False)\n",
        "    x1.append(pad_zeros(data[idx_0]))\n",
        "    x2.append(pad_zeros(data[idx_1]))\n",
        "\n",
        "#     x1.append(data[idx_0])\n",
        "#     x2.append(data[idx_1])\n",
        "\n",
        "    l1.append(data[idx_0].shape[0])\n",
        "    l2.append(data[idx_1].shape[0])\n",
        "    y.append([1])\n",
        "#     print(idx_0, idx_1, 1)\n",
        "    \n",
        "  # generate negative pairs\n",
        "  for _ in range(batch_size):\n",
        "    # randomly select idx_0 only if stick is false\n",
        "    if not stick:\n",
        "      base = np.random.randint(num_classes)\n",
        "    \n",
        "    # make sure neg_base is not same as base\n",
        "    while True:\n",
        "      neg_base = np.random.randint(num_classes)\n",
        "      if neg_base != base:\n",
        "        break\n",
        "      \n",
        "    idx_0 = base * samples_per_class + np.random.randint(samples_per_class)\n",
        "    idx_1 = neg_base * samples_per_class + np.random.randint(samples_per_class)\n",
        "    \n",
        "    x1.append(pad_zeros(train[idx_0]))\n",
        "    x2.append(pad_zeros(train[idx_1]))\n",
        "\n",
        "#     x1.append(train[idx_0])\n",
        "#     x2.append(train[idx_1])\n",
        "    \n",
        "    l1.append(train[idx_0].shape[0])\n",
        "    l2.append(train[idx_1].shape[0])\n",
        "    y.append([0])\n",
        "#     print(idx_0, idx_1, 0)\n",
        "    \n",
        "  return np.array(x1), np.array(x2), np.array(y), np.array(l1), np.array(l2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rVVbymY8cuAx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Create model"
      ]
    },
    {
      "metadata": {
        "id": "K2E9VhP7uqxf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_hidden_rnn = [500, 400]\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yDRUnq3hqgkN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Using this implementation as reference https://github.com/ardiya/siamesenetwork-tensorflow/blob/master/train.py\n",
        "\n",
        "def model(inp, seq_len):\n",
        "  cells = []\n",
        "  \n",
        "  for i, units in enumerate(num_hidden_rnn):\n",
        "    cell_name = 'lstm_' + str(i)\n",
        "    cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.GRUCell(units, reuse=tf.AUTO_REUSE, name=cell_name, kernel_initializer=tf.initializers.he_normal(), activation='relu'))\n",
        "    cells.append(cell)\n",
        "  \n",
        "  multi_rnn = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
        "  output, state = tf.nn.dynamic_rnn(multi_rnn, inp, dtype=tf.float32, sequence_length=seq_len)\n",
        "  print(output)\n",
        "      \n",
        "  flattened_1 = tf.contrib.layers.flatten(output)\n",
        "  dense_1 = tf.layers.dense(inputs=flattened_1, activation='relu', name='dense_1', reuse=tf.AUTO_REUSE, units=200)\n",
        "  dense_2 = tf.layers.dense(inputs=dense_1, activation='relu', name='dense_2', reuse=tf.AUTO_REUSE, units=50)\n",
        "    \n",
        "  return dense_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xCp67YQZDMnw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loss function"
      ]
    },
    {
      "metadata": {
        "id": "RrBRDdVunc4u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def contrastive_loss(model1, model2, y, margin):\n",
        "  # y can take values in {0, 1}\n",
        "\n",
        "  dotProduct = tf.reduce_sum(tf.multiply(model1, model2), axis = 1, keepdims=True)\n",
        "  sigmDotProduct = tf.nn.sigmoid(dotProduct)\n",
        "  lossCalcu = tf.reduce_sum(-y * tf.log(10e-8 + sigmDotProduct) - (1 - y) * tf.log(10e-8 + 1 - sigmDotProduct))\n",
        "\n",
        "  return lossCalcu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "evbRSpIYHNHj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_anchor = tf.placeholder(dtype=tf.float32, shape=[None, max_length, num_features])\n",
        "X_positive = tf.placeholder(dtype=tf.float32, shape=[None, max_length, num_features])\n",
        "X_negative = tf.placeholder(dtype=tf.float32, shape=[None, max_length, num_features])\n",
        "seq_len = tf.placeholder(dtype=tf.int32, shape=None)\n",
        "margin = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0lkc90kgIYcZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def triplet_loss(anchor, positive, negative, margin):\n",
        "  distance_ap = tf.sqrt(tf.reduce_sum(tf.pow(anchor - positive, 2), 1, keepdims=True))\n",
        "  distance_an = tf.sqrt(tf.reduce_sum(tf.pow(anchor - negative, 2), 1, keepdims=True))\n",
        "  return tf.reduce_mean(tf.maximum(distance_ap - distance_an + margin, 0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "enDutQ4AeRj0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "5367e723-bc73-4354-b0cb-eb7054dfecaf"
      },
      "cell_type": "code",
      "source": [
        "output_anchor = model(X_anchor, seq_len)\n",
        "output_positive = model(X_positive, seq_len)\n",
        "output_negative = model(X_negative, seq_len)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-29-c902e2307ec5>:10: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-29-c902e2307ec5>:11: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Tensor(\"rnn/transpose_1:0\", shape=(?, 50, 400), dtype=float32)\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-29-c902e2307ec5>:15: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "Tensor(\"rnn_1/transpose_1:0\", shape=(?, 50, 400), dtype=float32)\n",
            "Tensor(\"rnn_2/transpose_1:0\", shape=(?, 50, 400), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EY3bOF_ie95d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hFmScMV40XHB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def similar(model1, model2, y):\n",
        "  distance = tf.sqrt(tf.reduce_sum(tf.pow(model1 - model2, 2), 1, keepdims=True))\n",
        "  prob = 1 - tf.nn.sigmoid(distance)\n",
        "  prediction = tf.cast(tf.greater_equal(prob, 0.5), tf.float32)\n",
        "  accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, y), tf.float32))\n",
        "  return prob, prediction, accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1ewIvmzHF2UA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Inputs to the model"
      ]
    },
    {
      "metadata": {
        "id": "cWtiQaylvL-1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# X1 = tf.placeholder(dtype=tf.float32, shape=[None, max_length, num_features])\n",
        "# X2 = tf.placeholder(dtype=tf.float32, shape=[None, max_length, num_features])\n",
        "# y = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
        "# seq_len = tf.placeholder(dtype=tf.int32, shape=None)\n",
        "# margin = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3kljT8GaF9S2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Plug everything together"
      ]
    },
    {
      "metadata": {
        "id": "qcAm-hvPwqre",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# with tf.variable_scope(\"siamese\", reuse=tf.AUTO_REUSE) as scope:\n",
        "#   model1 = model(X1, seq_len)\n",
        "#   scope.reuse_variables()\n",
        "#   model2 = model(X2, seq_len)\n",
        "  \n",
        "# loss = contrastive_loss(model1, model2, y, margin)\n",
        "loss = triplet_loss(output_anchor, output_positive, output_negative, margin)\n",
        "train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sUYM0mghGEhb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ]
    },
    {
      "metadata": {
        "id": "keHumcYzrxBS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "num_epochs = 50\n",
        "display_step = 1\n",
        "save_step = 10\n",
        "\n",
        "num_samples_tr = train.shape[0]\n",
        "num_samples_te = test.shape[0]\n",
        "num_batches_tr = int(math.ceil(num_samples_tr/batch_size))\n",
        "num_batches_te = int(math.ceil(num_samples_te/batch_size))\n",
        "\n",
        "train_loss = []\n",
        "test_loss = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C1ViimzPsGvJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KAI0ND3tUuIc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "89fd647b-465f-441f-b1f0-ef34c06cabfd"
      },
      "cell_type": "code",
      "source": [
        "# for triplet loss\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "writer = tf.summary.FileWriter(LOG_DIR, sess.graph)\n",
        "writer.close()\n",
        "\n",
        "num_batches_tr = num_samples_tr // batch_size\n",
        "for epoch in range(num_epochs):\n",
        "  loss_val = 0\n",
        "  for i in range(num_batches_tr):\n",
        "    batch_a, batch_p, batch_n, l = next_batch_triplet(train, batch_size)\n",
        "    \n",
        "    _, lv = sess.run([train_op, loss], feed_dict={X_anchor: batch_a, X_positive: batch_p, X_negative: batch_n, seq_len: l})\n",
        "    loss_val += lv\n",
        "  \n",
        "  loss_val = loss_val / (num_batches_tr * batch_size)\n",
        "  \n",
        "  if epoch % display_step == 0:\n",
        "    print(epoch, loss_val)\n",
        "  if epoch % save_step == 0:\n",
        "    saver.save(sess, base_path_model + 'model_triplet' + str(epoch) + '.ckpt')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.9588335723876953\n",
            "1 0.8248589782714844\n",
            "2 0.6324112854003906\n",
            "3 0.48764279174804687\n",
            "4 0.46447966766357424\n",
            "5 0.45083017730712893\n",
            "6 0.3510453758239746\n",
            "7 0.31946418380737307\n",
            "8 0.3516475601196289\n",
            "9 0.344360725402832\n",
            "10 0.29226893615722654\n",
            "11 0.32848143768310545\n",
            "12 0.26575489807128905\n",
            "13 0.25913753509521487\n",
            "14 0.23579194259643554\n",
            "15 0.21867851638793945\n",
            "16 0.26555437850952146\n",
            "17 0.20687458229064942\n",
            "18 0.1851905689239502\n",
            "19 0.19142520141601563\n",
            "20 0.16517592239379883\n",
            "21 0.1376572437286377\n",
            "22 0.16139984035491944\n",
            "23 0.12215424919128418\n",
            "24 0.12420318794250489\n",
            "25 0.10813779306411743\n",
            "26 0.12283166217803955\n",
            "27 0.15044081687927247\n",
            "28 0.15938966178894043\n",
            "29 0.13903511428833007\n",
            "30 0.11258187770843506\n",
            "31 0.10412866687774658\n",
            "32 0.13179275894165038\n",
            "33 0.10649594020843506\n",
            "34 0.08577890634536743\n",
            "35 0.09392113399505615\n",
            "36 0.11077784252166747\n",
            "37 0.11232822608947754\n",
            "38 0.10640326881408692\n",
            "39 0.10094304656982422\n",
            "40 0.046883950233459475\n",
            "41 0.08909681749343872\n",
            "42 0.07644725894927978\n",
            "43 0.10269471549987794\n",
            "44 0.06101793003082275\n",
            "45 0.07679188776016235\n",
            "46 0.10186358261108398\n",
            "47 0.09423141479492188\n",
            "48 0.06418184280395507\n",
            "49 0.048220738887786864\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AF8-0ioesIc3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# for contrastive loss\n",
        "\n",
        "# sess.run(tf.global_variables_initializer())\n",
        "# writer = tf.summary.FileWriter(LOG_DIR, sess.graph)\n",
        "# writer.close()\n",
        "\n",
        "# num_batches_tr = num_samples_tr // batch_size\n",
        "# for epoch in range(num_epochs):\n",
        "#   loss_val = 0\n",
        "#   for i in range(num_batches_tr):\n",
        "#     batch_x1, batch_x2, batch_y, l1, l2 = next_batch(train, batch_size)\n",
        "    \n",
        "#     _, lv = sess.run([train_op, loss], feed_dict={X1: batch_x1, X2: batch_x2, y: batch_y, seq_len: l1})\n",
        "#     loss_val += lv\n",
        "  \n",
        "#   loss_val = loss_val / (num_batches_tr * batch_size)\n",
        "  \n",
        "#   if epoch % display_step == 0:\n",
        "#     print(epoch, loss_val)\n",
        "#   if epoch % save_step == 0:\n",
        "#     saver.save(sess, base_path_model + 'model_' + str(epoch) + '.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7t5VQBfH1D3O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test model"
      ]
    },
    {
      "metadata": {
        "id": "3w_LpP3n04cw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pb, pd, ac = sess.run([prob, prediction, accuracy], feed_dict={X1: batch_x1, X2: batch_x2, y: batch_y})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eqPzzuVF4WEo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}